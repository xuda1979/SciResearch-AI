\documentclass{article}
\usepackage[utf8]{inputenc}
\title{Causal Audit Trails for Transparent LLM Reasoning}
\author{Research Team}
\date{\today}

\begin{document}
\maketitle

\begin{abstract}
Causal Audit Trails (CAT) improve transparency and trust in language
model outputs by recording a directed acyclic graph of the sources,
tools, prompts, and code paths that influence each claim.
\end{abstract}

\section{Introduction}
Modern language models often provide answers without explicit
traceability. In regulated domains such as finance or medicine,
understanding the provenance of information is critical.

\section{Method}
We build a causal DAG for an answer where each node corresponds to a
source or operation and edges encode causal relationships with digests
and responsibility weights. By emitting a CAT alongside answers, users
and auditors can verify how each piece of information was derived.

\section{Implementation}
The accompanying \texttt{code.py} script constructs a toy causal audit
trail using the \texttt{sciresearch\_ai} module.

\section{Conclusion}
CATs provide a practical foundation for accountability, enabling
external verification of an answer's lineage and discouraging
unsupported claims.

\end{document}
